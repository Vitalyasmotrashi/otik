\input{commonpres}

\title{Передача информации}
% Предмет и основные разделы кибернетики. Формальное представление знаний. Виды информации. Хранение, измерение, обработка и передача информации. Базовые понятия теории информации. Способы измерения информации. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle





% \section{Информационный канал}


\subsection{Информационный канал}
\begin{frame}{\insertsubsection}
\footnotesize
\setlength{\parskip}{0\parskip}

\termin{Информационный канал}
 "--- совокупность устройств, объединённых линиями связи, предназначенных для передачи информации от источника информации (начального устройства канала) до её приёмника (конечного устройства канала).

\includegraphics[width=0.3\linewidth,valign=c]{07_channel}\centering

\begin{itemize}
\item достоверность передачи информации;
\item надёжность работы устройств;
\item скорость передачи информации;
\item задержка сигнала во времени.
\end{itemize}

\end{frame}



\subsection{Матмодель канала}
\begin{frame}{\insertsubsection}
\footnotesize
% \lstset{basicstyle=\ttfamily\footnotesize,aboveskip=0ex,belowskip=0ex}
% \lstset{xleftmargin=1em,numbers=none}
% \lstset{lineskip=0.5ex}
% \setbeamertemplate{itemize/enumerate body begin}{\footnotesize\setlistspacing{1}{0ex}}
\setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
\setbeamertemplate{itemize/enumerate subbody begin}{\footnotesize}
\setlength{\leftmargini}{0ex}
\setlength{\leftmarginii}{3ex}
\setlength{\parskip}{0.\parskip}

\begin{enumerate}
\item множество $X$ допустимых сообщений на входе;
\item множество $Y$ допустимых сообщений на выходе;
\item набор условных вероятностей $p(y|x)$ получения сигнала $y$ на выходе при  $x$ на входе
(статистические свойства шумов (помех)).
\end{enumerate}

Канал без шумов:
$X=Y$,
$p(y|x)= \left\{\begin{array}{ll} 
1,&  \text{при} ~ y=x\\
0,&  \text{при} ~ y\neq x\\
\end{array}\right.$

\hrulefill

% \end{frame}
% 
% \subsection{Дискретные и~непрерывные каналы}
% \begin{frame}{\insertsubsection}
% \setlength{\leftmargini}{0pt}
\begin{itemize}

\item В~\termin{дискретных} каналах сигналы на входе и~выходе "--- слова из одного или двух (вход и~выход) \termin{алфавитов.}
\item В~\termin{непрерывных} каналах входной и~выходной сигналы "--- функции от непрерывного параметра "--- времени. 
\item В~смешанных или гибридных каналах рассматривают их дискретные и~непрерывные компоненты раздельно.
\end{itemize}
\end{frame}


\section{Передача данных}

\subsection{Ёмкость канала}
\begin{frame}{\insertsubsection}
\footnotesize
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}

Способность канала передавать информацию характеризуется \termin{пропускной способностью} или \termin{ёмкостью} канала ($C$)
"--- максимальным количеством информации, передаваемым в~единицу времени.
$$C=\lim\limits_{T\rightarrow\infty}{  \dfrac{\max\limits_X \big(I(X,Y)\big)}{T}  }$$

Для канала без шума: % формула расчёта ёмкости канала имеет вид
$$C=\lim\limits_{T\rightarrow\infty}{  \dfrac{\max\limits_X \big(I(X)\big)}{T}  } 
=\lim\limits_{T\rightarrow\infty}{\log_2N(T)\over T},$$

где $N(T)$ "--- число всех возможных сигналов (сообщений)
за время $T$.

\end{frame}


\subsection{Единицы измерения}
\begin{frame}{\insertsubsection}
\footnotesize
% \lstset{basicstyle=\ttfamily\footnotesize,aboveskip=0ex,belowskip=0ex}
% \lstset{xleftmargin=1em,numbers=none}
% \lstset{lineskip=0.5ex}
% \setbeamertemplate{itemize/enumerate body begin}{\footnotesize\setlistspacing{1}{0ex}}
\setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
\setbeamertemplate{itemize/enumerate subbody begin}{\footnotesize}
\setlength{\leftmargini}{0ex}
\setlength{\leftmarginii}{3ex}
\setlength{\parskip}{0.\parskip}

Ёмкость "--- \termin{бод:}
\begin{itemize}
% Бод "--- единица измерения символьной скорости, количество изменений информационного параметра несущего периодического сигнала в~секунду. wiki
\item
«1 бод равен одному изменению информационного параметра в~секунду.
<...>
скорость в~бодах целиком определяется величиной такта»
(В.\,Г.\,Олифер, Н.\,А.\,Олифер  Компьютерные сети. Принципы, технологии, протоколы);
% Для двоичного кодирования без избыточности [бод]=[бит/с]

\item  «1 бод равно 0.8 бит/сек» (справочный портал \url{calc.ru});
% (\url{http://www.calc.ru/Peredacha-dannykh/bod-v-bit-sek.html});

\item «При пользовании нормальным компьютером бод эквивалентен количеству битов (bits) в~секунду» (Словарь бизнес-терминов).
\end{itemize}
% \end{frame}
% 
% \subsection{Информационная скорость}
% \begin{frame}{\insertsubsection}
% \setbeamertemplate{itemize/enumerate body begin}{\vspace{-0.9\baselineskip}}
% \setbeamertemplate{itemize/enumerate body end}{\vspace{-0.9\baselineskip}}

% \insertframetitle{}
% % Скорость передачи полезной информации
% измеряется в~битах в~секунду.

Информационная скорость "--- \termin{бит в~секунду} 
% (С.В. Кривальцевич):
(Олифер, Олифер):

\begin{itemize}
\item Если сигнал $\nu>2$ различимых состояний и~нет избыточности, то~каждое состояние несёт $\log_2 \nu$ бит, и~1~бод = $\log_2 \nu$ бит/с.
\item Если сигнал имеет 2 состояния и~для надёжности бит кодируется последовательностью из $\eta$ символов, то 1~бит/с = $\eta$ бод.

\end{itemize}
\end{frame}



\subsection{Первая теорема Шеннона (для канала без помех)}
\begin{frame}{\insertsubsection}
\small
\setlength{\leftmargini}{0ex}

\begin{enumerate}
\item  При любой производительности источника сообщений, меньшей пропускной способности канала: %, то есть при условии:
$$
\frac{H(X)}{T} < C
$$
% - сколь угодно малая положительная величина.
существует способ кодирования, позволяющий передавать по каналу все сообщения, вырабатываемые источником.

\item Не существует способа кодирования, обеспечивающего передачу сообщения без их неограниченного накопления, если
$$
\frac{H(X)}{T} > C
$$
% 
% Если пропускная способность канала связи больше энтропии источника информации в единицу времени ($C>H(X)$),
% то всегда можно закодировать достаточно длинное сообщение так, чтобы оно передавалось каналом связи без задержки. 
% 
% Если же, напротив, $C<H(X)$,
% то передача информации без задержек невозможна.
\end{enumerate}
\end{frame}






\section{Относительная информация}

\begin{frame}{\insertsection}
\footnotesize
% \lstset{basicstyle=\ttfamily\footnotesize,aboveskip=0ex,belowskip=0ex}
% \lstset{xleftmargin=1em,numbers=none}
% \lstset{lineskip=0.5ex}
% \setbeamertemplate{itemize/enumerate body begin}{\footnotesize\setlistspacing{1}{0ex}}
\setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
\setbeamertemplate{itemize/enumerate subbody begin}{\footnotesize}
\setlength{\leftmargini}{0ex}
\setlength{\leftmarginii}{3ex}
% \setlength{\parskip}{0.3\parskip}
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}
% \bigskip
По каналу связи передаются сообщения из~$X$. Из-за помех приёмником  воспринимается~$Y$.

$X$, $Y$ "--- источники сообщений:

\begin{tabularx}{1\linewidth}{@{}cL@{}}
\includegraphics[width=0.5\linewidth,valign=t]{01_Entropy-mutual-information-relative-entropy-relation-diagram}
&
$H(X) = I(X)$ "--- энтропия $X$

$H(Y) = I(Y)$ "--- энтропия $Y$

$I(X,Y)=I(Y,X)$ "--- \rlap{относительная} \mbox{информация $X$  и~$Y$}

$H(X,Y)=H(Y,X)$ "--- энтропия объединения $X$  и~$Y$

\end{tabularx}

$H(Y|X)$ "--- условная энтропия $Y$ относительно~$X$ (шум)
\\
$H(X|Y)$ "--- условная энтропия $X$ относительно~$Y$ (инф.\,потери)

% $H(x_i|y_j)$  "--- неопределённость того,  что,  отправив  $x_i$, мы получим $y_j$;
% 
% $H(y_j|x_i)$  "--- неуверенность после получения~$y_j$, что было отправлено  \rlap{$x_i$.}
% 
% $$
% H(X|Y) = M[-\log_2{p(X|Y)}] %=\\
% = \displaystyle- \sum_{i}\sum_{j} p(x_i, y_j)\cdot \log_2{p(x_i| y_j)}\leqslant H(X)
% $$
% $$
% H(Y|X) = M[-\log_2{p(Y|X)}] %=\\
% = \displaystyle- \sum_{i}\sum_{j} p(x_i, y_j)\cdot \log_2{p(y_j|x_i )}\leqslant H(Y)
% $$
\end{frame}


% \subsection{Относительная информация как связь}
% % \subsection{Дискретная относительная информация как связь}
% \begin{frame}{\insertsubsection}
% \footnotesize
% % \begin{adjustwidth}{-0em}{-1em}
% Для \termin{дискретных} случайных величин $X$ и~$Y$
% %, заданных 
% % законами распределения $P(X=X_i)=p_i$, $P(Y=Y_j)=q_j$ 
% % \\
% % и~совместным распределением {$P(X=X_i,Y=Y_j)=p_{ij}$,}
% % \\
% количество информации, содержащейся в~$X$ относительно $Y$, равно
% % $$I(X,Y)=\sum_{i,j}p_{ij}\log_2{p_{ij}\over p_iq_j}$$.
% $$I(X,Y)=\sum_{i}\sum_{j} p(x_i, y_j) \log_2{ \dfrac{p(x_i, y_j)}{p(x_i) \cdot p(y_j)} }$$
% 
% при этом
% $I(X,X)=H(X)$ и $I(X,X)=I(X)$.
% 
% % \end{adjustwidth}
% % \end{frame}
% % 
% % % % \section{Вероятностный подход к~измерению непрерывной информации}
% % % \subsection{Непрерывная относительная информация }
% % % \begin{frame}{\insertsubsection}
% % Для \termin{непрерывных} случайных величин $X$ и~$Y$, заданных плотностями распределения вероятностей $p_X(t_1)$, $p_Y(t_2)$ и~$p_{XY}(t_1,t_2)$, 
% % % аналогичная формула имеет вид
% % количество информации в~$X$ относительно $Y$
% % $$I(X,Y)=\iint\limits_{\Realset^2}  p_{XY}(t_1,t_2)\log_2{p_{XY}(t_1,t_2)\over p_X(t_1)p_Y(t_2)}dt_1dt_2
% % .$$
% 
% \end{frame}

\begingroup
% \setbeamerfont{frametitle}{size=\linespread{1.0}\normalsize}
\subsection{Условная энтропия как информационные потери}
\begin{frame}%[allowframebreaks]
{Условная энтропия как инф.\,потери}
\begin{adjustwidth}{-1.5em}{-1.8em}
\footnotesize
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}
% \scriptsize
\begin{tabularx}{1\linewidth}{@{}c>{\raggedright\arraybackslash}X@{}}
\includegraphics[width=0.25\linewidth,valign=t]{01_transmission_interference}%\centering\par
&

$H(x_i|y_j)$  "--- неопределённость того,  что,  отправив  $x_i$,  получим $y_j$;

$H(y_j|x_i)$  "--- неуверенность, которая остаётся после получения~$y_j$ в~том, что было отправлено именно $x_i$.
\end{tabularx}

$$
H(X|Y) = M[-\log_2{p(X|Y)}] %=\\
= \displaystyle- \sum_{i}\sum_{j} p(x_i, y_j)\cdot \log_2{p(x_i| y_j)}\leqslant H(X)
$$
$$
H(Y|X) = M[-\log_2{p(Y|X)}] %=\\
= \displaystyle- \sum_{i}\sum_{j} p(x_i, y_j)\cdot \log_2{p(y_j|x_i )}\leqslant H(Y)
$$
Если  помехи отсутствуют ($|Y|=|X|$, посланному $x_i$ соответствует принятый $y_i$),
% то всегда посланному $x_1$ соответствует принятый $y_1$,  $x_2  -  y_2, ..., x_n - y_n$.  Энтропии источника и~приёмника равны 
то $H(X)= H(Y)$, %.\\
условная энтропия $H(Y|X) = 0$.\\
% Энтропия объединения $H(X,Y)= H(X) = H(Y)$.

Если $Y$ не зависит от $X$, то $H(Y|X)= H(Y)$. % и $H(X,Y)= H(X) + H(Y)$.

% \pagebreak\normalsize
% 
% В общем случае энтропия объединённой системы   $$H(X,Y)\leqslant H(X)+H(Y),$$  $$H(Y/X)\leqslant H(Y)$$  
% 
% $$H(X,Y) = H(Y,X)$$


\end{adjustwidth}
\end{frame}
\endgroup



% \begingroup
% \subsection{Энтропия объединения% дискретных случайных величин
% }
% \setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
% \setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}
% 
% \begin{frame}%[allowframebreaks]
% % {Энтропия системы дискретных случайных величин}
% {\insertsubsection}
% \footnotesize
% \setlength{\parskip}{0\parskip}
% % \setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
% % \setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}
% 
% Энтропия объединения дискретных случайных величин $X$ и $Y$
% $$\begin{array}{c}
% H(X,Y)= M[-\log_2{p(X, Y)%p(x_i, y_j)
% }]
% %=\\
% =\displaystyle
% - \sum_{i}\sum_{j} p(x_i, y_j) \cdot \log_2{p(x_i, y_j)}
% \end{array}$$
% где $p(x_i, y_j)$ "--- вероятность совместного появления %$i$-го и $j$-го значений %случайных величин 
% % $X$ и $Y$.
% $x_i$ и~$y_j$.
% % \pagebreak
% % \bigskip
% $$
% % \begin{array}{c}
% H(X,Y) = H(Y,X) = H(X) + H(Y|X) = H(Y) + H(X|Y)\leqslant H(X)+H(Y)%\\[1ex]
% % \end{array}
% $$
% % \bigskip
% 
% Если  помехи отсутствуют% ($Y=X$, посланному $x_i$ соответствует принятый $y_i$)
% ,
% то 
% % $H(X)= H(Y)$, %.\\
% % условная энтропия $H(Y|X) = 0$.\\
% % Энтропия объединения 
% $H(X,Y)= H(X) = H(Y)$.
% \bigskip
% 
% % В общем случае  
% % $H(Y|X)\leqslant H(Y), H(X,Y) = H(Y,X)$
% % 
% % Энтропия системы зависимых величин:
% % 
% % \bigskip
% 
% Если $Y$ не зависит от $X$, то $H(X,Y)= H(X) + H(Y)$.
% \end{frame}
% \endgroup


\begingroup
\setbeamerfont{frametitle}{size=\linespread{1.0}\normalsize}
\subsection{Свойства меры информации и энтропии}
\begin{frame}%[allowframebreaks]
{\insertsubsection}
\begin{adjustwidth}{-1.5em}{-1.8em}
% \small
\begin{enumerate}
\item  $I(X,Y)\geqslant 0$,\\ $I(X,Y)=0 \Leftrightarrow$ $X$ и $Y$ независимы;
\item  $I(X,Y)=I(Y,X)$;
\item  $H(X)=0 \Leftrightarrow X$ "--- константа;
\item\label{item:Joint-entropy}  $I(X,Y)=H(X)+H(Y)-H(X,Y)$; % =$\\$= H(X) - H(X|Y) = H(Y) - H(Y|X)$
%, где $H(X,Y)=-\sum_{i}\sum_{j} p(x_i, y_j)\log_2 p(x_i, y_j)$;
\item  $I(X,Y)\leqslant I(X,X) = I(X) =H(X).$ 

Если $I(X,Y)=I(X,X)$, то $X$ "--- функция от $Y$ (разные $y$ при разных $x$, передача без потерь). 

% Если $X$ "--- инъективная функция от $Y$ (разные $y$ "--- в~разные $x$), то $I(X,Y)=I(X,X)$.
\end{enumerate}
\end{adjustwidth}
\end{frame}
% \pagebreak
\subsection{Взаимные энтропия и~информация}
\begin{frame}%[allowframebreaks]
{\insertsubsection}
\begin{adjustwidth}{-1.5em}{-1.8em}
\footnotesize
\begin{tabularx}{1\linewidth}{@{}c>{\raggedright\arraybackslash}X@{}}
\includegraphics[width=0.5\linewidth,valign=t]{01_Entropy-mutual-information-relative-entropy-relation-diagram}
&
К~свойству~\ref{item:Joint-entropy}:

$I(X,Y)=$

$ \llap{= }H(X)+H(Y)-H(X,Y) =$

$= H(X) - H(X|Y) =$
$= H(Y) - H(Y|X)$
\end{tabularx}

% Расчёт количества информации с использованием свойства~\ref{item:Joint-entropy}, а не определения, обычно требует меньше вычислений.
\vspace{-1em}


% $$I(X,Y)=\sum_{i}\sum_{j} p(x_i, y_j) \log_2{ \dfrac{p(x_i, y_j)}{p(x_i) \cdot p(y_j)} }$$
% % \vspace{-2em}
% 
% $$
% H(X|Y) %= M[-\log_2{p(X|Y)}] =
% = - \sum_{i}\sum_{j} p(x_i, y_j)\cdot \log_2{p(x_i| y_j)}
% $$

$$\begin{array}{c}
\displaystyle
I(X,Y)=\sum_{i}\sum_{j} p(x_i, y_j) \log_2{ \dfrac{p(x_i, y_j)}{p(x_i) \cdot p(y_j)} }
\\[3ex]
\displaystyle
H(X|Y) = M[-\log_2{p(X|Y)}] %=
= - \sum_{i}\sum_{j} p(x_i, y_j)\cdot \log_2{p(x_i| y_j)}
\\[3ex]
H(X,Y)= M[-\log_2{p(X, Y)%p(x_i, y_j)
}]
%=\\
=\displaystyle
- \sum_{i}\sum_{j} p(x_i, y_j) \cdot \log_2{p(x_i, y_j)}
\end{array}$$
% где $p(x_i, y_j)$ "--- вероятность совместного появления %$i$-го и $j$-го значений %случайных величин 
% $x_i$ и~$y_j$.


\end{adjustwidth}
\end{frame}
\endgroup




\section{Двоичный симметричный канал}
\subsection{Двоичный симметричный канал}

\begin{frame}{\insertsubsection}
\begin{adjustwidth}{-0em}{-1em}
\footnotesize
\setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
\setbeamertemplate{itemize/enumerate subbody begin}{\footnotesize}
\setlength{\leftmargini}{0ex}
\setlength{\leftmarginii}{3ex}
\setlength{\parskip}{0.\parskip}
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}

% \vspace*{-2\baselineskip}

\begin{tabularx}{1\linewidth}{@{}>{\raggedright\arraybackslash}Xc@{}}

От $X$ к~$Y$ передаются символы 0 и~1 ($k$~символов в~единицу времени).
\medskip

Каждый символ, независимо от других, с~вероятностью $\mu$ может быть искажён (т.\,е. заменён противоположным). 
% Требуется найти пропускную способность канала. 

&
\scriptsize
% \raisebox{4ex}[0mm][0mm]{
  \begin{tikzpicture}%[baseline=(current bounding box.north)]
  [baseline=(K-1-1.base)]
    \matrix (K)
    [matrix of math nodes,%
    ampersand replacement=\&,
    nodes={outer sep=0pt,circle,minimum size=4pt,draw},
    column sep={32ex,between origins},
    row sep={16ex,between origins}]
    {
      0 \& 0\\
      1 \& 1\\
    };
  \draw[arrowline] (K-1-1) -- node[auto,pos=0.8] {$\mu$} (K-2-2);
  \draw[arrowline] (K-2-1) -- node[auto,pos=0.2] {$\mu$} (K-1-2);
  
  \draw[arrowline] (K-1-1) -- node[auto] {$1-\mu$} (K-1-2);
  \draw[arrowline] (K-2-1) -- node[auto,swap] {$1-\mu$} (K-2-2);
  \end{tikzpicture}
% }
\end{tabularx}

Пусть $X$ производит %символы 
$x_1=0$ и $x_2=1$ с~вероятностями $p$ и~$1-p$,
на~выходе $Y$ символы $y_1=0$ и $y_2=1$ с~вероятностями $r$ и~$1-r$.
\medskip

$
H(Y|X) = p\cdot H(Y|x_1) + (1-p)\cdot H(Y|x_2)
$
\begin{adjustwidth}{-2.5em}{-2em}
% \begin{adjustwidth}{-1.em}{-2em}
% \scriptsize

$H(Y|x_1) = - \sum\limits_{i=1}^2 P(y_i| x_1)  \log_2 P(y_i| x_1) 
= - (1{-}\mu)\log_2(1{-}\mu) - \mu \log_2\mu 
$
\\
$H(Y|x_2) = - \sum\limits_{i=1}^2 P(y_i| x_2)  \log_2 P(y_i| x_2) 
=  - \mu  \log_2\mu - (1{-}\mu)\log_2(1{-}\mu) 
= H(Y|x_1) 
$
\end{adjustwidth}

$
H(Y|X) = p\cdot H(Y|x_1) + (1-p)\cdot H(Y|x_1)
=- \mu \cdot \log_2\mu - (1-\mu)\cdot\log_2(1-\mu)
$

\end{adjustwidth}
\end{frame}

\begingroup
\setbeamerfont{frametitle}{size=\linespread{1.0}\normalsize}

% \subsection{Макс. передаваемая информация на символ}
\subsection{Передаваемая информация}
\begin{frame}%[allowframebreaks]
{\insertsubsection}
\begin{adjustwidth}{-1em}{-1.5em}
% \small
\footnotesize
\setlength{\parskip}{0.\parskip}
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}

$H(Y) = -r\cdot \log_2 r -(1-r)\cdot \log_2 (1-r)$
\bigskip


Передаваемая информация на символ
% 
% $$
% I(X,Y)= H(Y) - H(Y|X) %= H(X) - H(Y|X)
% % $$
% % % $$
% % % H(X) = -p\cdot \log_2 p -(1-p)\cdot \log_2 (1-p)
% % % $$
% % $$
% , ~ \text{где} ~
% H(Y) = -r\cdot \log_2 r -(1-r)\cdot \log_2 (1-r)
% % \text{, где $r$ "--- вероятность 0 на выходе}
% $$

$
I(X,Y)= H(Y) - H(Y|X) =
$
$
=  \Big(-r\cdot \log_2 r -(1-r)\cdot \log_2 (1-r)\Big)  - \Big(  - \mu \cdot \log_2\mu - (1-\mu)\cdot\log_2(1-\mu)  \Big)
$
\bigskip

Обозначим $\eta(x) = -x\cdot \log_2 x$: $I(X,Y) = \big(  \eta(r) + \eta(1-r)  \big) - \big(  \eta(\mu) + \eta(1-\mu)  \big) $
\bigskip


Макс. передаваемая информация на символ\hfill
% $\max\limits_X \big(I(X,Y)\big)=$
\hfill\strut\\
% $
% =
% \max\limits_r
%  \Big(-r\cdot \log_2 r -(1-r)\cdot \log_2 (1-r)\Big)  -
% $
% $
%  - \Big(  - \mu \cdot \log_2\mu - (1-\mu)\cdot\log_2(1-\mu)  \Big)=
% $
% 
% $
% = 1 - \Big(  - \mu \cdot \log_2\mu - (1-\mu)\cdot\log_2(1-\mu)  \Big)=
% $\\
% 
$\max\limits_X \big(I(X,Y)\big) = 
\max\limits_X\Big(
\big(  \eta(r) + \eta(1-r)  \big) - \big(  \eta(\mu) + \eta(1-\mu)  \big) 
\Big)
=$
$ = \max\limits_r \Big(  \eta(r) + \eta(1-r)  \Big)
 - \Big(  \eta(\mu) + \eta(1-\mu)  \Big)
\hfill = \hfill
1 - \Big(  \eta(\mu) + \eta(1-\mu)  \Big)$
\bigskip

Пропускная способность: %дв.\,симметричного канала:
$$
C=k\cdot\max\limits_X \big(I(X,Y)\big) = k\cdot\Big(1 - \big(  \eta(\mu) + \eta(1-\mu)  \big) \Big)
$$

\end{adjustwidth}
\end{frame}


% % \subsection{Макс. передаваемая информация на символ}
% \subsection{Пропускная способность}
% \begin{frame}{\insertsubsection}
% % \pagebreak
% % Максимальная передаваемая информация на~символ \hfill
% Макс. передаваемая информация на символ
% $\max\limits_X \big(I(X,Y)\big)=$
% \hfill\strut\\
% % $
% % =
% % \max\limits_r
% %  \Big(-r\cdot \log_2 r -(1-r)\cdot \log_2 (1-r)\Big)  -
% % $
% % $
% %  - \Big(  - \mu \cdot \log_2\mu - (1-\mu)\cdot\log_2(1-\mu)  \Big)=
% % $
% % 
% % $
% % = 1 - \Big(  - \mu \cdot \log_2\mu - (1-\mu)\cdot\log_2(1-\mu)  \Big)=
% % $\\
% % 
% $ = 
% \max\limits_X\Big(
% \big(  \eta(r) + \eta(1-r)  \big) - \big(  \eta(\mu) + \eta(1-\mu)  \big) 
% \Big)
% =$
% $ = \max\limits_r \Big(  \eta(r) + \eta(1-r)  \Big)
%  - \Big(  \eta(\mu) + \eta(1-\mu)  \Big)
% =$
% $
% = 1 - \Big(  \eta(\mu) + \eta(1-\mu)  \Big)
% $
% 
% Пропускная способность: %дв.\,симметричного канала:
% $$
% C=k\cdot\Big(1 - \big(  \eta(\mu) + \eta(1-\mu)  \big) \Big)
% $$
% 
% 
% % \end{adjustwidth}
% \end{frame}

\section{Помехозащитное кодирование}

% http://fkn.ktu10.com/?q=forum/123
\begingroup
% \setbeamerfont{frametitle}{size=\linespread{1.0}\normalsize}
\subsection{Вторая теорема Шеннона (для канала с~помехами)}
\begin{frame}{\insertsubsection}
\small
% \footnotesize
\setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
\setbeamertemplate{itemize/enumerate subbody begin}{\footnotesize}
\setlength{\leftmargini}{0ex}
\setlength{\leftmarginii}{3ex}
\setlength{\parskip}{0.\parskip}
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}

\begin{enumerate}
% \item При любой производительности источника сообщений, меньшей, чем пропускная способность канала 
\item  При любой производительности источника сообщений, меньшей пропускной способности канала: %, то есть при условии:
$$
\frac{H(X)}{T} < C
$$    
    существует  способ кодирования,  позволяющий обеспечить передачу всей информации со \termin{сколь угодно малой вероятностью ошибки.}
    
% \item Если производительность источника сообщений больше, чем пропускная способность канала, то не существует способа кодирования, который позволил бы обеспечить передачу информации со сколь угодно малой вероятностью ошибки.
\item Не существует способа кодирования, обеспечивающего передачу информации со сколь угодно малой вероятностью ошибки, если
$$
\frac{H(X)}{T} > C
$$
\end{enumerate}

% Если производительность источника сообщений $H(X)$ меньше пропускной способности канала $C$:
% $H(X) < C$
% 
% то существует способ кодирования (преобразования сообщения в сигнал на входе) и декодирования (преобразования сигнала в сообщение на выходе канала), при котором вероятность ошибочного декодирования и ненадежность Н(А|A?) могут быть сколь угодно малы. Если же Н'(А)С, то таких способов не существует. 
% 
% Пусть источник характеризуется д.с.в. $X$. Рассматривается канал с~шумом, т.е. для каждого передаваемого сообщения задана вероятность его искажения в~процессе передачи (вероятность ошибки). 
% 
% Тогда существует такая скорость передачи $u$, зависящая только от $X$,\\ что $\forall\varepsilon>0\; \exists u'<u$ сколь угодно близкая к~$u$\\ такая, что существует способ передавать значения $X$ со скоростью $u'$ и~с~вероятностью ошибки меньшей $\varepsilon,$\\
% причём
% $u={C\over HX}.$
% 
% Упомянутый способ образует помехоустойчивый код.

\end{frame}
\endgroup

% \begin{frame}{Обратная теорема (Фэно)}
% % Кроме того, Фэно доказана1 следующая обратная теорема о~кодировании при наличии помех. 
% 
% Для $u'>u$ можно найти такое положительное число $\varepsilon,$\\
% что в~случае передачи информации по линии связи со скоростью $u'$ вероятность ошибки $\varepsilon_i$ передачи каждого символа сообщения при любом методе кодирования и~декодирования будет не меньше $\varepsilon$\\ ($\varepsilon$ очевидно растет вслед за ростом $u'$).
% 
% \end{frame}
\endgroup

% \subsection{Передача и хранение информации}
% \begin{frame}{\insertsubsection}
% \small
% 
% \begin{adjustwidth}{0em}{-2em}
% \begin{description}
% \item[кодер источника] "--- представление информации в~наиболее компактной форме;
% 
% \item[кодер канала] "--- защита от помех и~возможных искажений;
% 
% \item[модулятор] "--- преобразование в~сигналы среды.
% \end{description}
% \end{adjustwidth}
% 
% \includegraphics[width=1\linewidth,valign=t]{00_communications_system}\centering\par
% 
% % \begin{adjustwidth}{0em}{-2em}
% % Декодеры и~демодулятор "--- обратные операции и~восстановление
% % \end{adjustwidth}
% 
% \end{frame}

\subsection{Простейшие помехозащитные  коды}
\begin{frame}{\insertsubsection}
\setbeamertemplate{itemize/enumerate body begin}{\footnotesize}
\setbeamertemplate{itemize/enumerate subbody begin}{\footnotesize}
\setlength{\leftmargini}{0ex}
\setlength{\leftmarginii}{3ex}
\setlength{\parskip}{0.\parskip}
\begin{enumerate}
\item Обнаруживающий одиночную ошибку (здесь и~далее "--- инверсию) в~одном бите "--- двойное повторение каждого бита. 

\item Обнаруживающий одиночную ошибку 
в~блоке из $N$ бит "---  \termin{контроль чётности} (добавление к~каждому блоку $N+1$-го бита так, чтобы дополнить количество единиц до заранее выбранного для кода чётного (even) или нечетного (odd) значения).

Двойная ошибка в~блоке не будет обнаружена.

\item Исправляющий одиночную ошибку  в~одном бите "--- тройное повторение каждого бита. 
% \\
% Если в~одной тройке возникнет двойная или тройная ошибка, то будут получены неправильные данные.

\item Исправляющий одиночную ошибку в~блоке из $N$ бит "--- код Хэмминга.
% Если в~одном блоке возникнет двойная или тройная ошибка, то будут получены неправильные данные.

Двойная ошибка в~блоке будет 
% не обнаружена и~исправлена неверно.
принята за одиночную не в~том месте.

\item Исправляющий одиночную ошибку и~обнаруживающий двойную в~блоке из $N$ бит "--- \termin{код Хэмминга с~дополнительным битом чётности.}


\end{enumerate}
\end{frame}

% \subsection{Простейшие коды, обнаруживающие одиночную инверсию бита}
% \begin{frame}{\insertsubsection}
% 
% Простейший код, обнаруживающий одиночную ошибку в~одном бите "--- двойное повторение каждого бита. 
% 
% Простейший код, обнаруживающий ошибки в~блоке из $N$ бит "---  контроль чётности (добавление к~каждому блоку $N+1$-го бита так, чтобы дополнить количество единиц до заранее выбранного для кода чётного (even) или нечетного (odd) значения).
% 
% Если в~одном блоке возникнет двойная ошибка, то она не будет обнаружена.
% 
% \end{frame}
% 
% 
% \subsection{Простейшие коды, исправляющие одиночную инверсию бита}
% \begin{frame}{\insertsubsection}
% % Простейший код, обнаруживающий ошибки "---  контроль чётности (добавление к каждому байту девятого бита так, чтобы дополнить количество единиц до заранее выбранного для кода чётного (even) или нечетного (odd) значения).
% 
% Простейший код, исправляющий одиночную ошибку  в~одном бите "--- тройное повторение каждого бита. 
% \\
% Если в~одной тройке возникнет двойная или тройная ошибка, то будут получены неправильные данные.
% 
% Простейший код, исправляющий одиночную ошибку в~блоке из $N$ бит "--- код Хэмминга.
% \end{frame}




\makethanks
\end{document}
