\input{commonpath}
\input{\SRCROOTPATH/commonpres}

\title{Основы теории информации и~кодирования.
Измерение информации. Кодирование. \mbox{Форматы файлов}
}
% \title{Введение}
% Предмет и основные разделы кибернетики. Формальное представление знаний. Виды информации. Хранение, измерение, обработка и передача информации. Базовые понятия теории информации. Способы измерения информации. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{ marvosym }
% \author[\InstitutionShortLong, \CathedraShort, \MyAbbrev]{\mbox{\MyIofFull} / {\bfseries illinc\MVAt mail.ru} \mbox{+7-985-148-32-64}}
%\author[\InstitutionShort, \MyUnitShort, \MyAbbrev]{\mbox{\MyIofFull} / {\bfseries illinc\MVAt mail.ru} \mbox{+7-985-148-32-64 (телефон), +7-977-977-97-29 (WhatsApp)}}


\def\UnicodeVersion{12.1}
\def\UnicodeSymCount{137\,994}


\begin{document}
\author[\AlmaMaterAbbrev, \MyUnitAbbrev, \MyFioAbbrev]{\mbox{\MyIofFull} / {\bfseries illinc\MVAt mail.ru} \mbox{+7-985-148-32-64 (телефон), +7-977-977-97-29 (WhatsApp),}  \url{gitlab.com/illinc/raspisanie}}

\maketitle

% \section{Оргвопросы}
% \subsection{Компетенции}

% \begin{frame}{Компетенции}
% 
% % Общепрофессиональные:
% 
% 
% \termin{ПК-2.3} Знание и~умение применять языки и~методы формальных спецификаций
% 
% \terminblue 
% Участвует в~формировании профессиональной компетенции
% \\
% \termin{ПК-2}		
% Владение навыками использования операционных систем, сетевых технологий, средств разработки программного интерфейса, применения языков и методов формальных спецификаций, систем управления базами данных
% 
% \end{frame}
% 
% \begin{frame}{Компетенция}
% \small
% \termin{Компетенция} "--- компонент качества человека, определяющий его способность выполнять определенную группу действий в~сфере того или иного рода деятельности. Это некий уровень требований к~человеку, соответствующий его роли в~обществе. 
% 
% \setlistspacing{1}{0ex}
% 
% Особенность компетенции как результата обучения состоит в~том, что в~сравнении с~другими результатами образования она:
% \vspace{-1em}
% \begin{itemize}
% \item является интегрированным результатом, 
% \item позволяет решать целый класс нестандартных задач, 
% \item существует в~форме деятельности, 
% \item переносима (связана с~целым классом предметов воздействия), совершенствуется по пути интеграции с~другими компетенциями, 
% \item проявляется сознательно (в~отличие от навыка).
% \end{itemize}
% \end{frame}

% \begin{frame}{test}
% test
% \begin{enumerate}
% \item test
% \begin{enumerate}
% \item test
% 
% \end{enumerate}
% \end{enumerate}
% \end{frame}


% \subsection{Регламент}

\begin{frame}{Регламент}
\small
\setlength{\parskip}{0\parskip}
    \setlength{\abovedisplayskip}{0.5em}
    \setlength{\belowdisplayskip}{0.5em}
    
См.~\url{https://gitlab.com/illinc/otik/}
% -/blob/master/Регламент и КМ.txt
    
\smallskip
    
% Баллы (максимум):
% $$
% \begin{array}{c}
% 7\cdot8~(\text{л/р})+2\cdot9~(\text{сем})+2\cdot 4~(\text{КР})
% % \\
% +18~(\text{бонус/экзамен}) 
% % =\\
% = 100
% \end{array}
% $$
% % Допустимый минимум:
% % $$
% % \begin{array}{c}
% % 7\cdot4(\text{л/р})+2\cdot5(\text{сем})+3(\text{КР})
% % % \\
% % +9(\text{бонус/экзамен}) = 50
% % \end{array}
% % $$

Дополнительные баллы:
% \setlength{\itemsep}{0ex}
% \setlength{\parskip}{0.\parskip}
% \setlistspacing{1}{0ex}
% \vspace{-0.21em}
\begin{enumerate}\itemsep0ex\parsep0ex
\item бонусные задания л/р;
% % % \item выполнение всех л/р до 15 недели "--- $9$ баллов (до 17 "--- $7$);
% \item решение задач на лекциях "--- $2-3$ балла за задачу;
\item вычитка материала "--- $1-4$ балла за принятое замечание, $2-8$~за~принятое исправление%;
% \item пополнение списка литературы "--- $(-1)-(+8)$ баллов% за~статью/книгу
.
\end{enumerate}
\smallskip

Экзамен (оценка): базовые вопросы/билет
% \\\hfill
% \begin{tabular}{ll}
% \termin{$5$} ~~ $86-100$\\
% \termin{$4$} ~~ $70-85$\\
% \termin{$3$} ~~ $50-69$\\
% \termin{$2$} ~~ $0-49$\\
% \end{tabular}
% \hfill\strut

\vfill

Консультации 
"--- см.~\url{gitlab.com/illinc/raspisanie}
% \hfill\texttime{14:45}

\vfill


Посещаемость лекций не учитывается. Никогда более.

Баллы за посещаемость семинаров выставляются в~ОРИОКС на 9 и~15/17 неделях, не чаще.

\end{frame}


% \subsection{Выбор времени консультаций}
% \begin{frame}{\insertsubsection}
% 
% \begin{enumerate}
% \item Понедельник-числитель, 16:10 "--- после лекции
% (по~знаменателю консультация будет \mbox{в~среду или четверг).}
% 
% \item Среда, 19:20
% 
% \item Четверг, 14:30
% \end{enumerate}
% 
% 
% \end{frame}




\section{Предмет теории информации. Источник информации}




% \subsection{Теория информации и~связь}
% % \subsection{Теория информации и~кодирование}
\begingroup
\renewcommand{\tabularxcolumn}[1]{m{#1}}
\begin{frame}{\insertsection}
% \begin{frame}{\insertsubsection}
\begin{adjustwidth}{-0.5em}{-1.em}
% \small
\small
\setlength{\parskip}{0.5\parskip}

\termin{Теория информации} "---
математическая теория, посвящённая измерению информации, её~потока, «размеров» канала связи и т.\,п., особенно применительно к~%
% радио, телеграфии, телевидению и~к~другим 
средствам связи:
\\ 
% \hfill 
\hspace*{1em}
$x \hookleftarrow X  ~~\sim~~ I(x)$\hfill\strut
\\
% \hfill
% ($x$ "--- сообщение, $X$ "--- источник \rlap{[сл.\,процесс/\termin{сл.\,вел.}]).}
% (сообщения+вероятности)
$x$ "--- сообщение, $X = \{x, p(x)\}$ "--- источник \rlap{(сл.\,процесс/{сл.\,величина}).}
\\Дискретное $x$ может состоять из символов или быть отдельным \rlap{символом.}


\termin{Информация} "--- нематериальная сущность, при помощи которой с~любой точностью можно описывать реальные (материальные), виртуальные (возможные) и понятийные сущности.


\begin{tabularx}{1\linewidth}{@{}cL@{}}
$\begin{array}{@{}c@{}}I(x){:}\\[4ex]\strut\end{array}$&
\setlength{\leftmarginii}{1em}
% http://blog.nguyenvq.com/blog/2011/06/14/modify-spacings-in-listing-itemizeenumerate-environments-for-latex-beamer/
\let\olditem\item 
\renewcommand{\item}{\setlength{\itemsep}{0mm}\olditem}
\setbeamertemplate{itemize/enumerate body begin}{\vspace{-0.5\baselineskip}}
\begin{enumerate}
\item %Неизмеряемость информации в~быту (информация как новизна).
% \termin{Новизна} (неизмеряемость в~быту).
\textcolor{gray}{\textbf{Новизна} (неизмеряемость в~быту).}

% Количество информации в~сообщении зависит от~того, насколько ново это сообщение для получателя. 

\item \termin{Объёмный} (длина "--- измерение в~технике).

% Основан на подсчёте числа символов в~сообщении (связан только с~длиной сообщения и~не учитывает его содержания). 

\item\termin{Вероятностный} ({снятая неопределённость} "--- измерение в~ТИ).%
% Количество информации зависит от вероятности получения данного сообщения. 
% Причём чем больше вероятность события, тем меньше количество информации в~сообщении о~нём. 
\end{enumerate}
\end{tabularx}

% \vspace{-\baselineskip}
% 
% \termin{Кодирование} "--- преобразование дискретной информации:\\
% \hspace*{1.em}
% $x \hookleftarrow X  ~~\to~~ code(x) $
% % одним из следующих способов: 
% % \\
% \hfill
% ({сжатие, защита от шума,} шифрование).

% $\text{данные} \supseteq \text{информация} \supseteq \text{знания}$
$\text{данные} \supseteq \text{информация} \supset \text{знания}$,
\hfill
ОТИК: $\text{инф-я} = \text{данные} + \text{источник}$


\end{adjustwidth}
\end{frame}
\endgroup


\subsection{Виды источников информации}
\begin{frame}{\insertsubsection}
% \begin{adjustwidth}{-1.5em}{-1.8em}
\begin{adjustwidth}{-4.5em}{-4.8em}
\footnotesize
\setlength{\parskip}{0.\parskip}
\setlength{\leftmarginii}{2em}

По сообщениям: \begin{itemize}
\item дискретные (цифровые)/непрерывные (аналоговые);
\item дискретные: качественные/количественные.
% \item качественные: символы/строки (зависит от рассматриваемого алфавита).
\end{itemize}
Элемент качественной информации "--- \termin{символ} $a \in A$ (множество $A$ "--- алфавит);
конечная последовательность символов "--- \termin{слово} $x \in A^{+}$ (строка, фраза).
\medskip


Источник символов алфавита $A$ (можно прочитать строку):
% \begin{enumerate}
% \item случайная последовательность без ограничений;

% \item нестационарный источник "--- вероятность символа зависит от времени/номера позиции
% даже при одинаковом контексте;
\begin{itemize}
\item {стационарный} (вероятность символа не зависит от времени/позиции: только от контекста) 
% / \itemizeball{}
\item {нестационарный} (при сдвиге  вероятности меняются);
\end{itemize}

% \item случайная последовательность, где вероятность следующего символа определяется только его предысторией;

\begin{enumerate}
% \item \termin{марковский} источник (стац./нестац.) "---вероятность символа определяется состоянием; 
% состояние изменяется после порождения символа \textcolor{gray}{(новое состояние однозначно определяется предыдущим и~порождённым символом);}

\item \termin{марковский} источник (большинство: марк. $\subset$ стац.; но иногда «нестац.\,марк.») "--- вероятность символа определяется состоянием;  
состояние изменяется после порождения символа  \\\textcolor{gray}{(новое состояние однозначно определяется предыдущим и~порождённым символом);}

% \item 
марковский источник порядка $m$ "--- 
% состояние на $i$-м шаге зависит от~состояний на $m$ предыдущих шагах: $i-1, i-2,..., i-m$
% \\(в~простейшем случае "--- 
вероятность символа на $i$-м шаге зависит от~$m$ предыдущих символов: $i-1, i-2,\ldots, i-m$;

\item 
% источник без памяти "--- состояние неизменно; 
\termin{стационарный источник без~памяти} "--- вероятность символа $a \in A$ постоянна (равна $p(a)$);

\item \termin{равновероятный источник} "--- вероятность символа $a \in A$ постоянна и~одинакова для всех символов (равна 
% $\frac{1}{N}$, где $|A|=N$
$\frac{1}{|A|}$);
\end{enumerate}

равновероятный $\subseteq$ стационарный без~памяти $\subseteq$ стационарный марковский % $\subseteq$ стационарный

\end{adjustwidth}
\end{frame}





% \subsection{История}
\subsection{Энтропия и~информация}
\begin{frame}{\insertsubsection}
\begin{adjustwidth}{-0.5em}{-1em}
\small
\setlength{\parskip}{0.4\parskip}

1865 г. "--- %немецкий физик 
Рудольф Клаузиус ввёл в~статистическую физику понятие \termin{энтропии} "--- меры \rlap{уравновешенности  [Дж/К].} % системы.
% температуру тоже можно считать в джоулях
% энтропия безразмерна

1877 г. "--- Людвиг Больцман установил связь энтропии с~вероятностью. %данного состояния

1901 г. "--- Макс Планк определил энтропию как    $H=k\cdot\ln(\Omega)$, 
\mbox{где~$k$ "--- коэффициент Больцмана} [Дж/К].

1921 г. "--- %основатель большей части математической статистики, англичанин 
Роналд Фишер %впервые 
ввёл термин «информация» %в~математику
%, но полученные им формулы носят очень специальный характер
% http://inosmi.ru/world/20140809/222254208.html?id=222271176
% http://booksshare.net/index.php?id1=4&category=cybern&author=ashbiwr&book=1959&page=115
% 
% До него принималось как само собой разумеющееся, что как бы умен ни был статистик, более умный мог бы получить больше информации из имеющихся данных. Но сэр Рональд Фишер показал, что 
(информация, которую можно извлечь из имеющихся данных, \termin{имеет предел}).
% и~что задача каждого статистика — лишь приближаться к~этому максимуму, ибо выше него никто не может подняться. 
% (нельзя из имеющихся данных извлечь )

1928 г. "--- Ральф Хартли "--- логарифмическая мера информации для~\termin{равновероятных} событий.

1948 г. "--- Клод Шеннон %в~своих работах по теории связи 
% выписывает формулы для вычисления
"--- вычисление количества информация и~энтропии. 

% \termin{Основное соотношение:} % между энтропией и информацией:
% \hfill
% $I+\dfrac{\log_2 e}{k}H=const$ \hfill $\left(\dfrac{dI}{dt}= -\dfrac{\log_2 e}{k}\dfrac{dH}{dt} \right).$

{Основное соотношение}  между энтропией и информацией:
\\
$I+\dfrac{\log_2 e}{k}H=const$  ~ [бит]~~~~ $\left(\dfrac{dI}{dt}= -\dfrac{\log_2 e}{k}\dfrac{dH}{dt} ~~ \text{[бит/с]} \right).$ 

\end{adjustwidth}
\end{frame}







% \subsection{Базовые понятия теории информации}
% \begin{frame}{\insertsubsection}
% \small
% 
% \begin{adjustwidth}{-1em}{-2em}
% \begin{description}[Ш]
% 
% \item[Информация] "--- нематериальная сущность, при помощи которой с~любой точностью можно описывать реальные (материальные), виртуальные (возможные) и понятийные сущности. 
% 
% Информация "--- противоположность неопределённости.
% 
% \item[Канал связи] "---  среда передачи информации, которая характеризуется в~первую очередь максимально возможной для неё скоростью передачи данных (ёмкостью канала связи).
% 
% \item[Шум] "--- помехи в~канале связи при передаче информации.
% 
% \item[Кодирование] "--- преобразование дискретной информации одним из следующих способов: \\сжатие, защита от шума, шифрование.
% 
% 
% % Ёмкость канала связи без шума можно приблизительно вычислить, зная максимальную частоту волновых процессов, допустимую в этом канале. Можно считать, что скорость передачи данных может быть не меньше, чем эта частота. Например, при предельной частоте, равной 1000Гц, можно обеспечить скорость передачи данных не меньше 1Кбод.
% \end{description}
% \end{adjustwidth}
% \end{frame}





% % \section{Измерение информации}
% \subsection{Различные подходы к~измерению}
% \begin{frame}{\insertsubsection}
% % \begin{adjustwidth}{-1.5em}{-1.8em}
% % \small
% \setlength{\leftmargini}{0ex}
% 
% \begin{enumerate}
% \item %Неизмеряемость информации в~быту (информация как новизна).
% \termin{Новизна} (неизмеряемость в~быту).
% 
% Количество информации в~сообщении зависит от~того, насколько ново это сообщение для получателя. 
% 
% \item \termin{Объёмный} (измерение в~технике).
% 
% Основан на подсчёте числа символов в~сообщении (связан только с~длиной сообщения и~не учитывает его содержания). 
% 
% \item\termin{Вероятностный} (измерение  в~теории \rlap{информации}
% \\"--- информация как  \termin{снятая неопределённость}).
% 
% % Количество информации зависит от вероятности получения данного сообщения. 
% % Причём чем больше вероятность события, тем меньше количество информации в~сообщении о~нём. 
% 
% \end{enumerate}
% % \end{adjustwidth}
% \end{frame}

% \subsection{Различные меры информации}
% \begin{frame}{\insertsubsection}
% \begin{adjustwidth}{-2em}{-1.8em}
% 
% \setbeamerfont{itemize/enumerate body}{size=\small}
% \setbeamerfont{itemize/enumerate subbody}{size=\small}
% \begin{enumerate}
% \item Синтаксическая мера (энтропийная)
% \begin{itemize}
% \item объём данных $V$
% \item количество информации в~сообщении~$b$ $I_a(b) = H_a(0)-H_a(b)$
% \end{itemize}
% 
% \item Семантическая мера
% 
% \begin{tabularx}{1\linewidth}{@{}Lc@{}}
% \begin{itemize}
% % \item $I^C$ "--- количество семантической информации, воспринимаемой пользователем и включаемой им в дальнейшем в свой тезаурус
% \item тезаурус $S_p$ "--- способность пользователя принимать поступившее сообщение
% \item коэффициент содержательности $C_b = \frac{I_c(b)}{V}$
% % \\
% % тезаурусная мера семантической информации имеет максимум
% % \begin{itemize}
% % \item при $S_p\to 0$ пользователь не воспринимает, не понимает поступающую информацию;
% % \item при $S_p\to \infty$ пользователь всё знает, и~поступающая информация ему не нужна.
% % \end{itemize}
% \end{itemize}
% &
% \includegraphics[width=0.3\linewidth,valign=t]{02_semantinf}
% \end{tabularx}
% \item Прагматическая мера
% 
% \begin{itemize}
% \item полезность информации (ценность) для достижения пользователем поставленной цепи
% \end{itemize}
% 
% \end{enumerate}
% \end{adjustwidth}
% \end{frame}
% 
% 
% \subsection{Семантическая мера истинности}
% \begin{frame}{\insertsubsection}
% \setbeamertemplate{itemize/enumerate body begin}{\vspace{-0.9\baselineskip}}
% \setbeamertemplate{itemize/enumerate body end}{\vspace{-0.9\baselineskip}}
% 
% \begin{adjustwidth}{-1.5em}{-1.8em}\small
% Одна из мер информации из предложений естественного языка "--- 
% функция $inf(s)=-\log_2p(s)$, где $s$ "---  предложение, смысловое содержание которого измеряется, $p(s)$ "---  вероятность истинности $s$. 
% 
% Некоторые свойства $inf$:
% \begin{enumerate}
% \item     если $s_1 \Rightarrow s_2$ (из $s_1$ следует $s_2$)% "---  истинно
% , то $inf(s_1) \ge inf(s_2)$;
% \item    $inf(s) \ge 0$;
% \item    если $s$ "---  истинно, то $inf(s)=0$;
%  \item   $inf(s_1s_2)=inf(s_1)+inf(s_2) \Leftrightarrow p(s_1\cdot s_2)=p(s_1)p(s_2)$, 
%  
% \hfill т.\,е. независимости $s_1$ и~$s_2$. \hspace*{2em}
% \end{enumerate}
% 
% Значение этой функции-меры больше для предложений, исключающих большее количество возможностей. 
% 
% \end{adjustwidth}
% \end{frame}


% \subsection{Единица измерения информации}
% \begin{frame}{\insertsubsection}
% % http://www.5byte.ru/8/0003.php
% % Если некоторое сообщение приводит к~уменьшению неопределённости нашего знания, то  можно говорить, что 
% % такое сообщение содержит информацию.
% \termin{Количество информации} можно рассматривать как меру уменьшения неопределённости знания при получении информационных сообщений.
% 
% За единицу количества информации принимается такое количество информации, которое содержится в~сообщении, уменьшающем неопределённость знания в~два раза (бит).
% 
% \terminblue
% Источник с~двумя равновероятными состояниями.
% 
% \end{frame}


% \section{Измерение информации}

\begingroup
\subsection{Единица измерения информации}
% \subsection{Два равновероятных состояния}
\renewcommand{\tabularxcolumn}[1]{m{#1}}	% для копирования
\begin{frame}{\insertsubsection}
\begin{adjustwidth}{-3.5em}{-3.5em}
\small
% \setlength{\parskip}{0.5\parskip} 
\setlength{\parskip}{0.4\parskip} 

\termin{Бит} "--- количество информации в~сообщении, уменьшающем неопределённость знания в~два раза.

% \vfill

\termin{Источник с~двумя равновероятными состояниями}
"--- симметричная монета

% \renewcommand{\arraystretch}{1.5}
% Симметричная монета
\begin{tabularx}{1\linewidth}{lll}
\uncover<1->{\texttt{~.}} & \uncover<1->{?& %$7,5$ бит
 $2$ возможных варианта}
\\
\uncover<1->{\texttt{~Р}}& \uncover<1->{Решка}&\uncover<1->{$1$ вариант}\\\hline
\end{tabularx}
\uncover<1->{Неопределённость уменьшилась в~$2$~раза: $I(\text{\tt Р}) = 1$ бит}

% \end{frame}
% \subsection{Уменьшение неопределённости}
% \renewcommand{\tabularxcolumn}[1]{m{#1}}	% для копирования
% \begin{frame}{\insertsubsection}
% \renewcommand{\arraystretch}{1.5}
\begin{tabularx}{1\linewidth}{lll}
\uncover<1->{\texttt{..}} & \uncover<1->{Две симметричные монеты}& %$7,5$ бит
\\

\uncover<1->{\texttt{О.}}& \uncover<1->{Первая "--- вверх орлом}&\uncover<1->{$2$ раза ($+1$ бит)}\\

\uncover<1->{\texttt{ОР}}& \uncover<1->{Вторая "--- вверх решкой}& \uncover<1->{$2$ раза ($+1$ бит)}
\uncover<1->{\\\hline
 & $4$ возможных варианта& $I(\text{\tt ОР}) = 2$ бита}
\\

\end{tabularx}

% \hfill\texttime{15:00}
\vfill

{\terminblue Другая единица "--- \termin{трит:} троичный разряд; %кол-во инф-ции 
количество информации в~сообщении, уменьшающем неопределённость в~три раза; симметричная игральная кость D3.


$1 ~\text{трит} = \log_2 (3) \approx 1,6 ~\text{бита}$, 
~~
$1 ~\text{бит} = \log_3 (2) \approx 0,6 ~\text{трита}$.

}

Бит используется чаще трита только из-за двоичности базы ЭВМ, а~не из-за \rlap{свойств бита.}

\end{adjustwidth}
\end{frame}

% \subsection{Уменьшение неопределённости}
% \renewcommand{\tabularxcolumn}[1]{m{#1}}	% для копирования
% \begin{frame}{\insertsubsection}
% \renewcommand{\arraystretch}{1.5}
% \begin{tabularx}{1\linewidth}{lll}
% \uncover<1->{\texttt{......}} & \uncover<2->{175 слов}& %$7,5$ бит
% \\
% 
% \uncover<3->{\texttt{.а.и.а}}& \uncover<4->{122 слова}&\uncover<4->{$1,4$ раза ($+0,5$ бит)}\\
% 
% \uncover<5->{\texttt{р.б.т.}}& \uncover<6->{4 слова}& \uncover<6->{$43,8$ раз ($+5,5$ бит)}
% \end{tabularx}
% \end{frame}
\endgroup

% \subsection{Частные случаи}
% \begin{frame}{\insertsubsection}
% \small
% 
% \begin{adjustwidth}{-3em}{-2em}
% \begin{enumerate}
% % \item    Равенство вещественных переменных $a=b$, заключает в себе информацию о том, что $a$ равно $b$. Про равенство $a^2=b^2$ можно сказать, что оно несет меньшую информацию, чем первое, т.к. из первого следует второе, но не наоборот. Равенство $a^3=b^3$ несет в себе информацию по объёму такую же, как и первое;
% \item    Пусть происходят некоторые измерения с некоторой погрешностью. 
% 
% Тогда чем больше будет проведено измерений, тем больше информации об измеряемой сущности будет получено.
% 
% \item    Математическое ожидание некоторой случайной величины содержит в~себе информацию о самой случайной величине.
% % Для случайной величины, распределенной по нормальному закону, с известной дисперсией знание математического ожидания дает полную информацию о случайной величине;
% \item    Рассмотрим схему передачи информации. Пусть передатчик описывается случайной величиной $X$, тогда из-за помех в канале связи на приёмник будет приходить случайная величина $Y=X+Z$, где $Z$ "--- это случайная величина, описывающая помехи. В этой схеме можно говорить о~количестве информации, содержащейся в случайной величине $Y$ относительно $X$. 
% 
% Чем ниже уровень помех (дисперсия $Z$ мала), тем больше информации можно получить из $Y$. 
% 
% При отсутствии помех $Y$ содержит в себе всю информацию об $X$.
% \end{enumerate}
% \end{adjustwidth}
% \end{frame}



% А для низкой жизни были числа,
% Как домашний, подъярёмный скот,
% Потому, что все? оттенки смысла
% Умное число передаёт.
% \section{Хранение, измерение, обработка и передача информации}
% \section{Обработка и передача информации}
% \subsection{Единицы измерения информации}
% \begin{frame}{\insertsubsection}
% 
% \begin{adjustwidth}{-0.5em}{-2em}
% \small
% \setlength{\parskip}{0\parskip}
% \setlistspacing{1}{0ex}
% 
% \termin{Количество информации}
% \begin{enumerate}
% \item мера уменьшения неопределённости знания при получении сообщения;
% \item {\terminblue наименьший размер, до которого можно сжать сообщение.}
% \end{enumerate}
% 
% 
% 
% % Дискретная информация хранится в~виде последовательностей нулей и единиц.
% 
% \termin{Бит} "--- единица измерения количества информации, равная 
% \begin{enumerate}
% \item количеству информации в~сообщении, уменьшающем неопределённость знания в~два раза;
% \item {\terminblue одному разряду в двоичной системе счисления.}
% \end{enumerate}
% 
% \termin{Октет} "--- 8 бит ($2^8$ состояний).
% 
% \termin{Байт} "--- совокупность битов, обрабатываемая одномоментно (в~современных вычислительных системах байт=октет).
% 
% % Машинное \termin{слово}  "--- разрядность регистров процессора и/или шины данных.
% \end{adjustwidth}
% \end{frame}



\subsection{Требования к~мере информации $I(x)$}
\begin{frame}{\insertsubsection}
\small
\setbeamertemplate{itemize/enumerate body begin}{\small}
\setbeamertemplate{itemize/enumerate subbody begin}{\small}
\setlength{\leftmargini}{0ex} 

% \begin{adjustwidth}{-2em}{-2em}
%     \setlength{\abovedisplayskip}{0.ex}
%     \setlength{\belowdisplayskip}{0.5ex}
    
% % Измерение информации необходимо для оптимизации передачи по каналам связи $\to$ вероятностная мера информации $\mu(x)$ 
% $I(x)$
% должна быть монотонно связана с~затратами на передачу сообщения $x \hookleftarrow X$.
% \vspace{-1em}
\begin{enumerate}
\item $I(x) \geqslant 0$.
\item Вероятностный подход: $I(x) = f(p_x)$.

\item 
Объёмный подход: $I(x)$ монотонно связана с~затратами на передачу
\let\olditem\item 
\renewcommand{\item}{\setlength{\itemsep}{1ex}\olditem}
\begin{itemize}
\item два равновероятных сообщения %логично кодировать как 
"--- $0$ и~$1$ (1 бит),\\
четыре "--- %как 
$00$, $01$, $10$, $11$ (2 бита) и~т.\,д.:
% $$\mu(1/2) = 1, \;\; \mu(1/4) = 2, \;\; \mu(1/8) = 3, \ldots \;\;\to\;\; \mu(p^m) = m\cdot\mu(p)$$
\\
$f\left(\frac{1}{2}\right) = 1, \;\; f\left(\frac{1}{4}\right) = 2, \;\; f\left(\frac{1}{8}\right) = 3, \ldots$

\item %знание предыдущих сообщений не позволяет предугадать следующие  $\to$ 
затраты на передачу %последовательности 
независимых сообщений складываются:
$I(x_1, \ldots, x_n) = I(x_1) +  \ldots + I(x_n)$

\vspace{1ex}

при этом вероятности независимых событий умножаются 
%$f(p_1\cdot \ldots\cdot p_n) = f(p_1) +  \ldots + f(p_n)$.
$f(p_1\times \ldots\times p_n) = f(p_1) +  \ldots + f(p_n)$.
% \hfill\texttime{15:05}


\end{itemize}

\end{enumerate}
% \end{adjustwidth}
\end{frame}

% \section{Энтропия}












\section{Вероятностная мера информации}

\subsection{Формула Хартли для равновероятных событий}
% \subsection{Формулы Хартли и~Шеннона}

\begin{frame}{\insertsubsection}
\begin{adjustwidth}{-0.5em}{-1em}
% \setlength{\belowdisplayskip}{1000pt} \setlength{\belowdisplayshortskip}{1000pt} не работает
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}
\small
\setlength{\parskip}{0.1\parskip}

\vspace{1\baselineskip}


% Для \termin{равновероятных} событий:
% Количество информации $I$ в~одном из  $N$ возможных сообщений:
% $|X| = N$, вероятности $x \hookleftarrow X$ равны:
Источник $X$ порождает $N$ \termin{равновероятных} сообщений $x$ ($\forall x \hookleftarrow X:~ p_X(x) =p = \frac{1}{N}$).
\vspace{-0.5\baselineskip}

$$ 
\begin{array}{r@{}c@{}ll}
I_X(x) = I(X) = I &{}  = \log_2 N = {}& - \log_2 (p) ~\text{битов}, &\text{или $2^I = N$};
\\
\terminblue I & \terminblue{} = \log_3 N ={}&\terminblue - \log_3 (p) ~\text{тритов}, &\terminblue \text{или} ~3^I = N.
\end{array}
$$
где
$I_X(x)$ "--- количество информации в~сообщении $x$;\\
$I(X)$ "--- \termin{среднее} кол-во информации в~одном сообщении источника $X$.


\medskip


Если $N = 2$% (выбор из двух возможностей)
, то $I = 1$ бит.
~~
\rlap{{\terminblue Если $N=3$, то $I = 1$ трит. %$I= \log_3 N$ тритов, $3^I = N$.
}}
\medskip

% \renewcommand{\arraystretch}{1.5}
Подбрасывание монеты
\begin{tabularx}{1\linewidth}{lll}
\texttt{..} & 4 варианта& $2$ бита
\\
\end{tabularx}

Угадывание слов по словарю
\begin{tabularx}{1\linewidth}{lll}
\texttt{......} & 175 слов& $7,5$ бит
\\

{\texttt{.а.и.а}}& {122 слова}&{$6,9$ бит}\\

{\texttt{р.б.т.}}&{4 слова}& $2$ бита
\end{tabularx}

% \bigskip\texttime{15:15}

% 1948 г. "--- %Клод Шеннон %в~своих работах по теории связи 
% % выписывает формулы для вычисления количества информация и~энтропии. 
% К.\,Шеннон для \termin{неравновероятных} событий:
% $$
% I(x) = - \log_2 (p_x)
% $$
\end{adjustwidth}
\end{frame}

% \subsection{Формула Шеннона}
% \begin{frame}{\insertsubsection{} для события}
% 1948 г. К.\,Шеннон для \termin{неравновероятных} событий:
% $$
% I(x) = - \log_2 (p_x)
% $$
% где       $p_x$ "--- вероятность появления $x$.
% 
% За меру количества информации принимается  \termin{энтропия} с~обратным знаком.\\
% {
% \small
% Чем больше энтропия системы, тем больше степень её неопределённости. Поступающее сообщение полностью или частично снимает эту неопределённость. Следовательно, количество информации можно измерять тем, насколько понизилась энтропия системы после поступления сообщения.
% 
% }
% \end{frame}




\subsection{Формула Шеннона для неравновероятных событий}
\begin{frame}{\insertsubsection}
% \small
    \setlength{\abovedisplayskip}{0.ex}
    \setlength{\belowdisplayskip}{0.ex}
\setlength{\parskip}{0\parskip}
\small

\begin{adjustwidth}{-0.5em}{-1em}
Количество информации $I$ в~сообщении с~вероятностью~$p_X(x)$:
$$
I_X(x) = - \log_2 p_X(x) ~\text{битов} ~~ {\terminblue  = - \log_3 p_X(x) ~\text{тритов}}
$$

Свойства:%\vspace{-1em}
\begin{enumerate}
\item Неотрицательность: $I_X(x) \geqslant 0, x \hookleftarrow X$.
\item Монотонность: \rlap{$x_1, x_2 \hookleftarrow X, p_X(x_1) \geqslant p_X(x_2) \to I_X(x_1) \leqslant I_X(x_2)$.}
\item Аддитивность: для независимых сообщений $x_1, \ldots, x_n$
$I_X(x_1, \ldots, x_n) = \sum\limits_{i=1}^{n}I_X(x_i)$
\item Для равновероятных событий соответствует формуле Хартли.
\end{enumerate}
\medskip

Среднее количество информации дискретного источника $X = \{x, p_X(x)\}$:
{
$
I(X) = \sum\limits_{x_i\hookleftarrow X} \Big( p_X(x_i)\cdot I_X(x_i) \Big) = - \sum\limits_{x_i\hookleftarrow X} \Big( p_X(x_i)\cdot \log_2 p_X(x_i)\Big) ~\text{битов}
$\centering

}

% \vfill
% 
% {\terminblue $I(x) = - \log_3 p(x) ~\text{тритов}$
% }
% 
\end{adjustwidth}

\end{frame}


\subsection{Количество информации в~тексте}

\begin{frame}{\insertsubsection}
\setlength{\leftmargini}{0ex} 

Из источника символов $X$ можно прочитать текст $\vec x = x_1 x_2 \ldots x_k$
\begin{enumerate}
\item Источник без памяти: сообщения $x_1, x_2, \ldots x_k$ независимы

$p(\vec x) = p(x_1) \cdot p(x_2) \cdot\ldots \cdot p(x_k)$ 

$I(\vec x) = I(x_1) + I(x_2) + \ldots + I(x_k)$

\bigskip

\item Источник с~памятью:

$p(\vec x) = p(x_1) \cdot p(x_2|x_1) \cdot\ldots \cdot p(x_k| x_1 x_2 \ldots x_{k-1})$ 

$I(\vec x) = I(x_1) + I(x_2|x_1) + \ldots + I(x_k| x_1 x_2 \ldots x_{k-1})$

\medskip

Если источник марковский порядка $m$:

$I(\vec x) = I(x_1) + \ldots + I(x_i| x_{i-m} \ldots x_{i-1})  + \ldots+ I(x_k| x_{k-m} \ldots x_{k-1})$


\end{enumerate}
\end{frame}


% \subsection{Энтропия и~информация}
% \begin{frame}{\insertsubsection}
% \small
% % \setlength{\parskip}{0.5\parskip}
% 
% Нулевой   
% энтропии
%    соответствует   максимальная   информация.   
%    
%    Основное соотношение: % между энтропией и информацией:
% $I+\dfrac{\log_2 e}{k}H=const$
% % или в дифференциальной форме
% $\left(\text{или~} \dfrac{dI}{dt}= -\dfrac{\log_2 e}{k}\dfrac{dH}{dt} \right).$
% 
% При переходе от состояния $H_1$  с~информацией $I_1$ к~$H_2$ с~$I_2$ возможны:
% \vspace{-1.\parskip}
% \begin{enumerate}
% \item $H_1 < H_2 ~ (I_1 >I_2)$ "--- уничтожение (уменьшение) старой информации в~системе; 
% \item $H_1 = H_2 ~ (I_1 = I_2 )$ "--- сохранение информации; 
% \item $H_1 > H_2 ~ (I_1 < I_2 )$
% "--- рождение новой (увеличение) информации.
% \end{enumerate}
% 
% % При правильном выборе единиц измерения $\dfrac{dI}{dt}= -\dfrac{dH}{dt}.$
% \end{frame}





% \section{Модель источника}


\subsection{Модель источника: $X$ неизвестен}
\begin{frame}{\insertsubsection}
\begin{adjustwidth}{-1.4em}{-1.5em}

% \def\msg{\chi}
\def\msg{x}


\small
\setbeamertemplate{itemize/enumerate body begin}{\small}
\setbeamertemplate{itemize/enumerate subbody begin}{\small}
\setlength{\leftmargini}{0em} 
% \setlength{\leftmarginii}{0.5em} 
\setlength{\leftmarginii}{1em} 

\setlength{\parskip}{\smallskipamount} 
\smallskip

Оценка алфавита $A_1$ и~вероятностей %$p(a)$ 
источника по сообщению: 
% обороноспособность (18)
\rlap{$\msg = $\termin{«молоко»}}%
% (6) $ = c_1, c_2, c_3, c_4, c_5, c_6$

\begin{enumerate}

\item $A_1$ "--- koi-8, равновероятные символы: $p =
% p(\text{к}) = p(\text{л})  = p(\text{м})= p(\text{о}) = 
\frac{1}{256}$, $I(\msg) = 6\cdot \log_2(256) = 48$ \rlap{(бит)}
\smallskip

\item $A_1$ "--- русский алфавит, равновероятные: $p%(\text{к}) = \ldots 
= \frac{1}{33}$, $I(\msg) = 6\cdot \log_2(33) \approx 30,3$ 
\smallskip

\item $A_1$ "--- Unicode {\UnicodeVersion}, равновероятные: $p = \frac{1}{\UnicodeSymCount}, I(\msg)\approx 6\cdot 17,1  \approx 102,4$ 
\smallskip

\item $A_1 = \{\text{к}, \text{л}, \text{м}, \text{о}\}$, равновероятные: 
% $p(\text{к}) = \ldots = \frac{1}{4}$
$p = \frac{1}{4}$, $I(\msg) = 6\cdot \log_2(4) = 12$ 
\medskip

\item $A_1 = \{\text{к}, \text{л}, \text{м}, \text{о}\}$ или koi-8, неравновероятные, стац-й источник без памяти:
%\termin{Источник без памяти} "--- предполагаем $p(c_i = a) = p(a) = const$ для~всех $i$ и~всех $a \in A_1$:

о (3) + к (1) + л (1) + м (1): \hfill $p(\text{о}) = \frac{3}{6}, \hfill p(\text{к}) = p(\text{л})  = p(\text{м})= \frac{1}{6}$

$I(\msg) = - 3\cdot \log_2(\frac{3}{6}) - \log_2(\frac{1}{6})- \log_2(\frac{1}{6})- \log_2(\frac{1}{6}) = 3\cdot \log_2(2) + 3\cdot \log_2(6)
\approx 10,8$%
\medskip

\item 
$A_1 = \{\text{к}, \text{л}, \text{м}, \text{о}\}$ или koi-8, 
% неравновероятные символы, память на 1~символ:
марковский ист%очни
-к первого порядка \rlap{с~вероятностями:}
% $p(c_i=a) = p(a,c_{i-1})$:

$
\begin{array}{@{}l|c|c|c|c|@{}}
% c_{i-1}&p(c_i=\text{к})&p(c_i=\text{л})&p(c_i=\text{м})&p(c_i=\text{о})\\\hline
\text{предыдущий}&p(\text{к})&p(\text{л})&p(\text{м})&p(\text{о})\\\hline
-&\frac{1}{4}&\frac{1}{4}&\frac{1}{4}&\frac{1}{4}\\\hline
\text{к, л, м}&0&0&0&1\\\hline
% \text{л}&0&0&0&1\\\hline
% \text{м}&0&0&0&1\\\hline
\text{о}&\frac{1}{2}&\frac{1}{2}&0&0\\\hline
\end{array}
$\hfill$\begin{array}{@{}c@{}}
I(\msg) = - \log_2(\frac{1}{4}) - \log_2(1) -\\
- \log_2(\frac{1}{2})- \log_2(1)-\\- \log_2(\frac{1}{2})- \log_2(1) =\\ = 2 + 1 + 1 = 4
\end{array}
$\hfill\strut
\smallskip
\item $A_1 = \{\text{молоко}, \text{чай}\}$, равновероятные символы: $p = \frac{1}{2}, I(\msg) = 1$ 
\smallskip

\item $A_1 = \{\text{молоко}\}$: $p = 1, I(\msg) = 0$ 
\end{enumerate}

% всего: 
% о (7) + с (3) + б (2) + н (2) + п (1) + р (1) + т (1) + ь (1)

% полагаем 
% $p(\text{о}) = \frac{7}{18}, p(\text{с}) = \frac{3}{18}, p(\text{б}) = p(\text{н}) = \frac{2}{18}, p(\text{п}) = p(\text{р}) = p(\text{т}) = p(\text{ь}) = \frac{1}{18}$

% \termin{Источник с~памятью} "--- предполагаем $p(c_i = a) = p(a, c_{i-1}, c_{i-2}, \ldots)$

% после «о»: 
% об, ор, он, ос (3): $p(\text{с}|\text{о}) = \frac{3}{6}, p(\text{б}|\text{о}) = p(\text{р}|\text{о}) =p(\text{н}|\text{о}) =\frac{1}{6}$\\
% $p(\text{о}|\text{о}) = p(\text{п}|\text{о}) =p(\text{т}|\text{о})=p(\text{ь}|\text{о}) =0$

% бо, бн:
% 
% ро:
% 
% но (2):
% 
% сп, со, ст:
% 
% по:
% 
% ть:



\end{adjustwidth}

% \termin{Источники данных} порождают только элементы,  \termin{физические} источники информации "--- символы или элементы.
\end{frame}




\section{Задачи: измерение информации}


\subsection{Задачи (равновероятный источник)}
\begin{frame}{\insertsubsection}

\begin{enumerate}
\item Найти количество информации в~событии «три~разные симметричные монеты выпали все вверх решкой».

\item Найти количество информации в~источнике  «три~разные симметричные монеты».
\end{enumerate}


\end{frame}

\subsection{Задачи (стационарный источник без памяти)}
\begin{frame}{\insertsubsection{}}
% \small
% \setbeamertemplate{itemize/enumerate body begin}{\small}
% \setbeamertemplate{itemize/enumerate subbody begin}{\small}
% \setlength{\leftmargini}{0ex} 

\begin{enumerate}
\item 
Найти количество информации в~событии «две из трёх неразличимых симметричных монет выпали вверх решкой, третья "--- орлом».
% \hfill\texttime{15:30}

\item
Найти количество информации в~источнике  «три~неразличимые симметричные монеты».
% \end{enumerate}

\bigskip

% \end{frame}
% 
% \begin{frame}{Задачи (2)}

% \begin{enumerate}
\item Найти количество информации в~событии «из~урны с~3~белыми и~5~чёрными шарами извлекли чёрный шар».

\item
Найти количество информации в~событии «из~урны с~3~белыми и~5~чёрными шарами извлекли белый шар».

\item
Найти количество информации в~источнике  «урна с~3~белыми и~5~чёрными шарами».
% \hfill\texttime{15:40}
\end{enumerate}


\end{frame}


\subsection{Задачи (стационарный источник с~памятью)}
\begin{frame}{\insertsubsection{}}
\small
\setbeamertemplate{itemize/enumerate body begin}{\small}
\setbeamertemplate{itemize/enumerate subbody begin}{\small}
\setlength{\leftmargini}{0ex} 
\setlength{\parskip}{0.5\parskip}

\begin{enumerate}
\item Источник $X$ генерирует последовательность подстрок «хрю» и~«мяу» (с~равной вероятностью), не разделяя их
\mbox{(например, «хрюхрюхрюмяухрюмяумяухрюмяумяу...»).}
Из~случайного места последовательности (не обязательно с~начала подстроки) читается три символа подряд
(сообщение $x$).

Найти количество информации в~событии «$x=\text{рюх}$».

\item Источник $X$ аналогично генерирует посл-ть «ку» и~«кукареку» \mbox{(например, «кукукукукарекукукукарекукукарекукукукарекукукареку...»).}
Из~случайного места посл-ти читается два (три) символа 
подряд ($x$).
% (сообщение~$x$).
% (два последовательных символа, если не сказано иное).

Найти количество информации в~событиях:
\setlength{\parskip}{0.\parskip}
% \begin{itemize}
% \item $x=\text{ка}$;
% 
% \item $x=\text{кар}$ (если было прочитано три символа);
% 
% \item $x=\text{ку}$;
% 
% \item $x=\text{ек}$.
% \end{itemize}
\begin{tabularx}{1\linewidth}{@{}LLL@{}}
\begin{itemize}
\item $x=\text{ка}$;

\item $x=\text{кар}$;
\end{itemize}
&
\begin{itemize}
\item $x=\text{ку}$;
\item $x=\text{ук}$;
\end{itemize}
&
\begin{itemize}
\item $x=\text{ек}$.
\end{itemize}
\end{tabularx}
\vspace{-1\baselineskip}

\textcolor{gray}{Подсказка: основная проблема в~том, что часть символов "--- одинаковые. Пусть они разные...}

\textcolor{gray}{Или: пусть всего $2N>>1$ слов, то есть $N$~«ку» и~$N$~«кукареку»...}
\end{enumerate}


\end{frame}



\subsection{Задачи (построение модели источника)}
\begin{frame}{\insertsubsection}
\small

Оценить алфавит и~построить модели источника: а) равновероятную, б) стационарную без памяти, в) марковскую первого порядка для сообщения $x$,
по модели оценить  $I(x)$ и~$I(y)$.

\begin{enumerate}

\item $x = \text{хрюхрюхрюмяухрюмяумяухрюмяумяу}$ (30 символов, 5~«хрю» (0) и~5 «мяу» (1) 0001011011); $y=\text{рюх}$.

\textcolor{gray}{В~тексте 5 двухбуквенных сочетаний, начинающихся с~«ю»: 2~«юх» и~3~«юм»}

\item $x = \text{кукукукукарекукукукарекукукарекукукукарекукукареку}$ (50~символов, 5~«ку» (0) и~5~«кукареку» (1) аналогично); $y=\text{кар}$.

\end{enumerate}

\end{frame}







% \section{Коды и~структура данных}
% \input{\ROOTPATH/pres-sections/00_ss_code.tex}


\section{Кодирование и~структуры данных}
\begin{frame}{\insertsection}
\small
\setlength{\parskip}{\smallskipamount} 
    \setlength{\abovedisplayskip}{0.5\abovedisplayskip}
    \setlength{\belowdisplayskip}{0.5\belowdisplayskip}


\termin{Кодирование} "--- преобразование дискретной информации 
$$x \hookleftarrow X, x \in A_1^+ ~~\to~~ code(x) \in A_2^+$$

% одним из следующих способов: 
% % \\
% \begin{enumerate}
% 
% \item \termin{сжатие;} 
% \item \termin{защита от помех (шума);} 
% \item шифрование.
% 
% \end{enumerate}

смена алфавита, \termin{сжатие, защита от шума,} шифрование.

% \begin{enumerate}
% 
% \item поточное;
% 
% \item блочное.
% 
% \end{enumerate}
\bigskip

\termin{Декодирование} "--- обратное преобразование $code(x) \to x$
\bigskip

% $A_1$ "--- первичный алфавит,
% $A_2$ "--- вторичный.

$x$ "--- сообщение, исходный текст, исходная строка,
% (не~обязательно текст или строка%; обычно $A_1$ "--- множество байтов
% ), 
блок;

$X$ "--- источник сообщений;

$A_1$ "--- первичный алфавит (до преобразования);

$A_2$ "--- вторичный (алфавит конечного представления).
\bigskip

Обычно $A_1$ "--- байты, исходные тексты $x$ "--- бинарные файлы.


\end{frame}

\subsection{Характеристики кодов}
\begin{frame}{\insertsubsection}
\small
\setbeamertemplate{itemize/enumerate body begin}{\small}
\setbeamertemplate{itemize/enumerate subbody begin}{\small}
\setlength{\leftmargini}{0ex} 

% Свойства кодов
\begin{enumerate}
% \item Перв\tikz[remember picture] \node[coordinate,yshift=0em] (n0) {};ичный 
% и~вторичный алфавиты ($A_2 = \{0, 1\}$ "--- двоичный код) 

\item Первичный алфавит $A_1$\tikz[remember picture] \node[coordinate,yshift=0.5em] (n1) {}; 
        
\item Оптимальность (неизбыточность) 
\item Избыточность (в том числе помехоустойчивость) \tikz[remember picture] \node[coordinate] (n2) {};

\item Вторичный алфавит $A_2$ ($A_2 = \{0, 1\}$ "--- двоичный код) 
\item Однозначная декодируемость [должна быть!]

\item Разделяемость
"---
код $code(x)$ любой последовательности \rlap{$x = \overline{a_1\ldots a_n}% \in A_1^+
$}\\
единственным образом разделим на кодовые слова
\rlap{$c_i = code(a_i), a_i \in A_1$:}

% Возможно однозначно выделить коды отдельных символов первичного алфавита.
\begin{enumerate}
\item коды фиксированной ширины "--- $a, b, c \to {}$ $00$, $01$, $10$;
\item коды с~разделителем "--- $1$, $11$, $111$ ($0$ как разделитель символов);
\item префиксные коды (дерево) "--- $0$, $10$, $11$; %"--- ни один $c_i$ не является началом другого
\item прочие "--- например, $11, 1110111, 11100111$.
% \hfill\texttime{16:00}

\end{enumerate}
\end{enumerate}

% ? собирать 2 раза, а то скобка не там
  \begin{tikzpicture}[overlay,remember picture]
%       \path (n2) -| node[coordinate] (n3) {} (n1);
      \draw[thick,decorate,decoration={brace,amplitude=3pt}]
%             (n1) -- (n3) 
            (n1-|n2) -- (n2) 
            node[midway, right=4pt] (text) {модель источника!};
%         \draw[blockarrow] (text.west) --  (n0);
  \end{tikzpicture}



\end{frame}








% \section{Сжатие}


% \subsection{Следствия из теоремы Шеннона}
\subsection{Теорема Шеннона для сжатия}
\begin{frame}{\insertsubsection}
\small
% \setlength{\parskip}{\smallskipamount} 
\setlength{\parskip}{0.3\parskip}
%     \setlength{\abovedisplayskip}{0.5\abovedisplayskip}
%     \setlength{\belowdisplayskip}{0.5\belowdisplayskip}


{Первая теорема Шеннона (для~сжатия)}:
% $|code(x)|\geqslant H(x)$
% $|code(X)|\geqslant H(X)$
$|code(X)|\geqslant I(X)$

\termin{$\mathrm{N}\!\!\mathrm{B}{:}$ усреднение по источнику $X$!}

При отсутствии помех 
% \termin{средняя длина двоичного кода} символа первичного алфавита может быть сколь угодно близкой к~\termin{средней информации,} приходящейся на знак первичного алфавита. 
{средняя длина кода}  может быть сколь угодно близкой к~{средней информации} сообщения.

\vspace{-0.5\baselineskip}

\hrulefill

\small
\setlength{\parskip}{0.\parskip}

Следствия:

\begin{enumerate}

\item не существует архиватора, который любой файл сжимает до 8~байт;

\item не существует архиватора, который любой блок из 9~байт сжимает до 8 байт.

\item не существует и такого архиватора, который любой блок из~$N+1$ бит сжимает ровно до $N$ бит, ни при каком $N$.

\end{enumerate}

\vspace{-0.5\baselineskip}

\hrulefill

Кодирование с~$|code(X)|\to I(X)$ и~$|code(x)|\to I(x)$ "--- \termin{оптимальное.}

\end{frame}



\subsection{Оптимальное кодирование источника $X$}
\begin{frame}{\insertsubsection}
% \begin{adjustwidth}{-1.em}{-1.5em}
\small
\setlength{\leftmargini}{0em} 
\setlength{\parskip}{0.3\parskip} 

Пусть $X$ порождает последовательность из $2^N$ возможных символов.

\begin{enumerate}
\item Равновероятный источник ($I(X)=N$) "--- кодирование отдельных символов кодами фиксированной ширины $N$ бит.
% (нет избыточности).

\item Стационарный источник без~памяти, порождающий символы с~разными постоянными вероятностями ($I(X)<N$) "--- кодирование \termin{отдельных символов} кодами переменной ширины: энтропийное кодирование (коды Хаффмана, методы~семейства арифметического кодирования) без учёта контекста.

\item Стационарный источник с~памятью, порождающий символы с~вероятностями, зависящими от контекста ($I(X)<N$) "--- кодирование \termin{сочетаний символов:} энтропийное кодирование (Хф и А) с~учётом контекста, словарные методы \mbox{семейства LZ77 (словарь=текст)} и~семейства LZ78 (отдельный словарь в~виде дерева/таблицы).

\end{enumerate}

Если изначально каждый символ записан кодом фиксированной ширины из $N$ бит $\Rightarrow$ сжатие для \enumilike{2} и~\enumilike{3}.

% Степень сжатия $\frac{|X|}{|code(X)|}$ оценивается на некоторой выборке файлов;

% \end{adjustwidth}
\end{frame}





\input{\SRCROOTPATH/pres-sections/00_code_header.tex}





% \section{Представление данных}
% \input{\ROOTPATH/pres-sections/00_code_header.tex}
\section{Простые коды (1)}


\input{\SRCROOTPATH/pres-sections/01_simplecodes.tex}






% \section{}
% % \begin{frame}{Вопросы}
% \subsection{Вопросы}
% \begin{frame}{\insertsubsection}
% % \setlength{\leftmargini}{0ex}
% \begin{enumerate}
% \small
% 
% % \item Какова цель курса?
% 
% \item Какие вы знаете способы измерения информации?
% 
% 
% \item Чем отличаются качественные и~количественные данные?
% 
% \item Что такое символ?
% 
% \item Какова оптимальная длина кода символа?
% 
% \item Какова оптимальная средняя длина кода символа?
% 
% 
% \item Какова оптимальная средняя длина сообщения?
% 
% \item Какие вы знаете единицы измерения информации?
% 
% \end{enumerate}
% 
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \author[\AlmaMaterAbbrev, \MyUnitAbbrev, \MyAbbrev]{\mbox{\MyIofFull,} \mbox{+7-985-148-32-64}}

% \makethanks

\section*{}
\begin{frame}{Спасибо за внимание!}

\insertinstitute\\ \AlmaMaterUrl{}\bigskip

\insertauthor\\
\url{https://gitlab.com/illinc/otik/}\bigskip

\end{frame}




\end{document}
